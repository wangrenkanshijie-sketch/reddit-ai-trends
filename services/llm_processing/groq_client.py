"""
Groq API Client

This module provides functionality to interact with the Groq API for LLM processing.
"""

import os
import logging
from datetime import datetime, timedelta
import groq
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv
from config import LLM_CONFIG
import re

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GroqClient:
    """Client for interacting with the Groq API."""
    
    def __init__(self):
        """Initialize the Groq API client using credentials from environment variables."""
        self.api_key = os.getenv('GROQ_API_KEY')
        if not self.api_key:
            raise ValueError("Groq API key not found in environment variables")
        
        self.client = groq.Client(api_key=self.api_key)
        self.model = LLM_CONFIG['model']
        self.temperature = LLM_CONFIG['temperature']
        self.max_tokens = LLM_CONFIG['max_tokens']
        
        logger.info(f"Groq API client initialized with model: {self.model}")
    
    def generate_text(self, 
                     prompt: str, 
                     temperature: Optional[float] = None, 
                     max_tokens: Optional[int] = None) -> str:
        """
        Generate text using the Groq API.
        
        Args:
            prompt: The prompt to send to the model
            temperature: Optional temperature override
            max_tokens: Optional max tokens override
            
        Returns:
            Generated text
        """
        # Use provided parameters or defaults
        temp = temperature if temperature is not None else self.temperature
        tokens = max_tokens if max_tokens is not None else self.max_tokens
        
        logger.info(f"Generating text with model: {self.model}, temperature: {temp}, max_tokens: {tokens}")
        
        try:
            # Call the Groq API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that provides accurate and factual information."},
                    {"role": "user", "content": prompt}
                ],
                temperature=temp,
                max_tokens=tokens
            )
            
            # Extract the generated text
            generated_text = response.choices[0].message.content
            
            # Remove <think></think> tags and their content using regex
            generated_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL)
            
            logger.info(f"Successfully generated text ({len(generated_text)} chars)")
            return generated_text
        
        except Exception as e:
            logger.error(f"Error generating text: {e}")
            raise
    
    def analyze_posts(self, posts: List[Dict[str, Any]]) -> str:
        """
        Analyze a list of Reddit posts and generate insights.
        
        Args:
            posts: List of post dictionaries
            
        Returns:
            Analysis text
        """
        if not posts:
            return "No posts to analyze."
        
        # Create a summary of the posts
        post_summaries = []
        for i, post in enumerate(posts[:20], 1):  # Limit to 20 posts to avoid token limits
            summary = (
                f"{i}. Title: {post.get('title', 'No title')}\n"
                f"   Subreddit: r/{post.get('subreddit', 'unknown')}\n"
                f"   Score: {post.get('score', 0)}, Comments: {post.get('num_comments', 0)}\n"
                f"   Category: {post.get('category', 'Other')}\n"
                f"   URL: {post.get('permalink', 'No URL')}\n"
                f"   Content: {post.get('selftext', '')[:200]}{'...' if len(post.get('selftext', '')) > 200 else ''}\n"
            )
            post_summaries.append(summary)
        
        post_summary_text = "\n".join(post_summaries)
        
        # Create the prompt
        prompt = f"""
        You are an AI assistant tasked with analyzing recent trending posts from Reddit AI and technology communities.
        
        Below are summaries of {len(posts)} recent posts:
        
        {post_summary_text}
        
        Please provide a comprehensive analysis of these posts, including:
        
        1. Key Trends: Identify 3-5 major trends or themes emerging from these posts.
        2. Notable Discussions: Highlight the most significant discussions or debates.
        3. New Technologies: Identify any new technologies, tools, or models mentioned.
        4. Community Interests: What topics seem to be generating the most interest?
        5. Technical Insights: Extract valuable technical insights or solutions shared.
        
        Format your response as a well-structured report with clear sections and bullet points where appropriate.
        Focus on being factual, concise, and informative. Avoid speculation and stick to what's evident from the posts.
        """
        
        # Generate the analysis
        return self.generate_text(prompt)
    
    def _create_monthly_popular_table(self, posts: List[Dict[str, Any]], previous_report: Optional[Dict[str, Any]] = None) -> str:
        """
        Create a markdown table of popular posts from the last month.
        
        Args:
            posts: List of post dictionaries
            previous_report: Optional dictionary containing previous report data for comparison
            
        Returns:
            Markdown table as a string
        """
        # Sort posts by score (descending)
        sorted_posts = sorted(posts, key=lambda x: x.get('score', 0), reverse=True)
        
        # Take top 20 posts
        top_posts = sorted_posts[:20]
        
        # Get previous monthly posts if available
        previous_monthly_posts = []
        if previous_report and 'posts_data' in previous_report:
            previous_monthly_posts = previous_report.get('monthly_posts', [])
        
        # Create lookup for previous rankings
        previous_rankings = {}
        for i, post in enumerate(previous_monthly_posts, 1):
            post_id = post.get('post_id')
            if post_id:
                previous_rankings[post_id] = i
        
        # Create table header
        table = "## Monthly Popular Posts\n\n"
        table += "| # | Title | Community | Score | Comments | Category | Posted | Previous Rank |\n"
        table += "|---|-------|-----------|-------|----------|----------|--------|---------------|\n"
        
        # Add rows to table
        for i, post in enumerate(top_posts, 1):
            title = post.get('title', 'N/A')
            
            # Enhanced title processing to prevent Markdown formatting issues
            # 1. Replace pipe characters that would break table formatting
            title = title.replace('|', '&#124;')
            
            # 2. Escape all brackets to prevent Markdown link interpretation
            title = title.replace('[', '\\[').replace(']', '\\]')
            
            # 3. Escape quotes to prevent string interpretation issues
            title = title.replace('"', '\\"').replace("'", "\\'")
            
            # 4. Replace any newline characters or carriage returns that might be in the title
            title = title.replace('\n', ' ').replace('\r', ' ')
            
            # 5. Replace periods followed by spaces with periods and non-breaking spaces
            # to prevent line breaks after periods
            title = title.replace('. ', '.&nbsp;')
            
            # Truncate long titles
            if len(title) > 60:
                title = title[:57] + "..."
            
            # Get community (subreddit) information
            subreddit = post.get('subreddit', 'N/A')
            subreddit_url = f"https://www.reddit.com/r/{subreddit}"
            community_link = f"[r/{subreddit}]({subreddit_url})"
            
            score = post.get('score', 0)
            comments = post.get('num_comments', 0)
            category = post.get('link_flair_text', 'N/A')
            if not category or category == 'None':
                category = 'General'
            
            # Format the timestamp
            created_utc = post.get('created_utc')
            if created_utc:
                if isinstance(created_utc, str):
                    try:
                        created_utc = datetime.fromisoformat(created_utc.replace('Z', '+00:00'))
                    except ValueError:
                        created_utc = datetime.utcnow()
                posted_time = created_utc.strftime("%Y-%m-%d %H:%M UTC")
            else:
                posted_time = 'N/A'
            
            # Get previous rank
            post_id = post.get('post_id', '')
            previous_rank = previous_rankings.get(post_id, 'New')
            
            # Create post URL
            post_url = f"https://www.reddit.com/comments/{post_id}"
            
            # Add row to table
            table += f"| {i} | [{title}]({post_url}) | {community_link} | {score} | {comments} | {category} | {posted_time} | {previous_rank} |\n"
        
        return table
    
    def _create_weekly_popular_table(self, posts: List[Dict[str, Any]], previous_report: Optional[Dict[str, Any]] = None) -> str:
        """
        Create a markdown table of popular posts from the last week.
        
        Args:
            posts: List of post dictionaries
            previous_report: Optional dictionary containing previous report data for comparison
            
        Returns:
            Markdown table as a string
        """
        # Sort posts by score (descending)
        sorted_posts = sorted(posts, key=lambda x: x.get('score', 0), reverse=True)
        
        # Take top 20 posts
        top_posts = sorted_posts[:20]
        
        # Get previous weekly posts if available
        previous_weekly_posts = []
        if previous_report and 'posts_data' in previous_report:
            previous_weekly_posts = previous_report.get('weekly_posts', [])
        
        # Create lookup for previous rankings
        previous_rankings = {}
        for i, post in enumerate(previous_weekly_posts, 1):
            post_id = post.get('post_id')
            if post_id:
                previous_rankings[post_id] = i
        
        # Create table header
        table = "## Weekly Popular Posts\n\n"
        table += "| # | Title | Community | Score | Comments | Category | Posted | Previous Rank |\n"
        table += "|---|-------|-----------|-------|----------|----------|--------|---------------|\n"
        
        # Add rows to table
        for i, post in enumerate(top_posts, 1):
            title = post.get('title', 'N/A')
            
            # Enhanced title processing to prevent Markdown formatting issues
            # 1. Replace pipe characters that would break table formatting
            title = title.replace('|', '&#124;')
            
            # 2. Escape all brackets to prevent Markdown link interpretation
            title = title.replace('[', '\\[').replace(']', '\\]')
            
            # 3. Escape quotes to prevent string interpretation issues
            title = title.replace('"', '\\"').replace("'", "\\'")
            
            # 4. Replace any newline characters or carriage returns that might be in the title
            title = title.replace('\n', ' ').replace('\r', ' ')
            
            # 5. Replace periods followed by spaces with periods and non-breaking spaces
            # to prevent line breaks after periods
            title = title.replace('. ', '.&nbsp;')
            
            # Truncate long titles
            if len(title) > 60:
                title = title[:57] + "..."
            
            # Get community (subreddit) information
            subreddit = post.get('subreddit', 'N/A')
            subreddit_url = f"https://www.reddit.com/r/{subreddit}"
            community_link = f"[r/{subreddit}]({subreddit_url})"
            
            score = post.get('score', 0)
            comments = post.get('num_comments', 0)
            category = post.get('link_flair_text', 'N/A')
            if not category or category == 'None':
                category = 'General'
            
            # Format the timestamp
            created_utc = post.get('created_utc')
            if created_utc:
                if isinstance(created_utc, str):
                    try:
                        created_utc = datetime.fromisoformat(created_utc.replace('Z', '+00:00'))
                    except ValueError:
                        created_utc = datetime.utcnow()
                posted_time = created_utc.strftime("%Y-%m-%d %H:%M UTC")
            else:
                posted_time = 'N/A'
            
            # Get previous rank
            post_id = post.get('post_id', '')
            previous_rank = previous_rankings.get(post_id, 'New')
            
            # Create post URL
            post_url = f"https://www.reddit.com/comments/{post_id}"
            
            # Add row to table
            table += f"| {i} | [{title}]({post_url}) | {community_link} | {score} | {comments} | {category} | {posted_time} | {previous_rank} |\n"
        
        return table
    
    def _create_trending_posts_table(self, posts: List[Dict[str, Any]]) -> str:
        """
        Create a markdown table of trending posts from the last 24 hours.
        
        Args:
            posts: List of post dictionaries
            
        Returns:
            Markdown table as a string
        """
        # Sort posts by score (descending)
        sorted_posts = sorted(posts, key=lambda x: x.get('score', 0), reverse=True)
        
        # Filter posts with more than 10 comments
        filtered_posts = [post for post in sorted_posts if post.get('num_comments', 0) > 10]
        
        # Take top 10 posts
        top_posts = filtered_posts[:10]
        
        # Create table header
        table = "## Trending Posts - Last 24 Hours\n\n"
        table += "| Title | Community | Score | Comments | Category | Posted |\n"
        table += "|-------|-----------|-------|----------|----------|--------|\n"
        
        # Add rows to table
        for post in top_posts:
            title = post.get('title', 'N/A')
            
            # Enhanced title processing to prevent Markdown formatting issues
            # 1. Replace pipe characters that would break table formatting
            title = title.replace('|', '&#124;')
            
            # 2. Escape all brackets to prevent Markdown link interpretation
            title = title.replace('[', '\\[').replace(']', '\\]')
            
            # 3. Escape quotes to prevent string interpretation issues
            title = title.replace('"', '\\"').replace("'", "\\'")
            
            # 4. Replace any newline characters or carriage returns that might be in the title
            title = title.replace('\n', ' ').replace('\r', ' ')
            
            # 5. Replace periods followed by spaces with periods and non-breaking spaces
            # to prevent line breaks after periods
            title = title.replace('. ', '.&nbsp;')
            
            # Truncate long titles
            if len(title) > 60:
                title = title[:57] + "..."
            
            # Get community (subreddit) information
            subreddit = post.get('subreddit', 'N/A')
            subreddit_url = f"https://www.reddit.com/r/{subreddit}"
            community_link = f"[r/{subreddit}]({subreddit_url})"
            
            score = post.get('score', 0)
            comments = post.get('num_comments', 0)
            category = post.get('link_flair_text', 'N/A')
            if not category or category == 'None':
                category = 'General'
            
            # Format the timestamp
            created_utc = post.get('created_utc')
            if created_utc:
                if isinstance(created_utc, str):
                    try:
                        created_utc = datetime.fromisoformat(created_utc.replace('Z', '+00:00'))
                    except ValueError:
                        created_utc = datetime.utcnow()
                posted_time = created_utc.strftime("%Y-%m-%d %H:%M UTC")
            else:
                posted_time = 'N/A'
            
            # Create post URL
            post_id = post.get('post_id', '')
            post_url = f"https://www.reddit.com/comments/{post_id}"
            
            # Add row to table
            table += f"| [{title}]({post_url}) | {community_link} | {score} | {comments} | {category} | {posted_time} |\n"
        
        return table
    
    def _create_long_term_popular_table(self, posts: List[Dict[str, Any]], previous_report: Optional[Dict[str, Any]] = None) -> str:
        """
        Create a table of popular posts from the last week/month with comparison data.
        
        Args:
            posts: List of post dictionaries
            previous_report: Previous report data for comparison
            
        Returns:
            Markdown table as a string
        """
        # Get posts from the last week
        week_ago = datetime.utcnow() - timedelta(days=7)
        weekly_posts = []
        
        for post in posts:
            created_utc = post.get('created_utc')
            if created_utc:
                if isinstance(created_utc, str):
                    try:
                        created_utc = datetime.fromisoformat(created_utc.replace('Z', '+00:00'))
                    except ValueError:
                        continue
                
                if created_utc >= week_ago:
                    weekly_posts.append(post)
        
        # Sort weekly posts by score (descending)
        weekly_posts = sorted(weekly_posts, key=lambda x: x.get('score', 0), reverse=True)
        
        # Take top 5 weekly posts
        top_weekly = weekly_posts[:5]
        
        # Create table header
        table = "## Popular Posts (Last Week)\n\n"
        table += "| Title | Current Score | Previous Score | Change | Current Comments | Previous Comments | Change |\n"
        table += "|-------|---------------|----------------|--------|------------------|-------------------|--------|\n"
        
        # Add rows to table
        for post in top_weekly:
            title = post.get('title', 'N/A')
            if len(title) > 50:
                title = title[:47] + "..."
            
            current_score = post.get('score', 0)
            current_comments = post.get('num_comments', 0)
            
            # Get previous data if available
            previous_score = 0
            previous_comments = 0
            
            post_id = post.get('post_id', '')
            if previous_report and 'posts_data' in previous_report:
                for prev_post in previous_report['posts_data']:
                    if prev_post.get('post_id') == post_id:
                        previous_score = prev_post.get('score', 0)
                        previous_comments = prev_post.get('num_comments', 0)
                        break
            
            # Calculate changes
            score_change = current_score - previous_score
            comments_change = current_comments - previous_comments
            
            # Format changes
            if score_change > 0:
                score_change_str = f"+{score_change} ðŸ“ˆ"
            elif score_change < 0:
                score_change_str = f"{score_change} ðŸ“‰"
            else:
                score_change_str = "0"
            
            if comments_change > 0:
                comments_change_str = f"+{comments_change} ðŸ“ˆ"
            elif comments_change < 0:
                comments_change_str = f"{comments_change} ðŸ“‰"
            else:
                comments_change_str = "0"
            
            # Create post URL
            post_url = f"https://www.reddit.com/comments/{post_id}"
            
            # Add row to table
            table += f"| [{title}]({post_url}) | {current_score} | {previous_score} | {score_change_str} | {current_comments} | {previous_comments} | {comments_change_str} |\n"
        
        return table
    
    def _create_community_top_posts_tables(self, posts: List[Dict[str, Any]]) -> str:
        """
        Create tables showing top 3 posts from each community for the past week.
        
        Args:
            posts: List of post dictionaries
            
        Returns:
            Markdown tables as a string
        """
        # Filter posts from the last week
        week_ago = datetime.utcnow() - timedelta(days=7)
        weekly_posts = []
        
        for post in posts:
            created_utc = post.get('created_utc')
            if created_utc:
                if isinstance(created_utc, str):
                    try:
                        created_utc = datetime.fromisoformat(created_utc.replace('Z', '+00:00'))
                    except ValueError:
                        continue
                
                if created_utc >= week_ago:
                    weekly_posts.append(post)
        
        # Group posts by subreddit
        communities = {}
        for post in weekly_posts:
            subreddit = post.get('subreddit')
            if not subreddit:
                continue
                
            if subreddit not in communities:
                communities[subreddit] = []
            
            communities[subreddit].append(post)
        
        # Sort communities alphabetically
        sorted_communities = sorted(communities.keys())
        
        # Create tables for each community
        all_tables = "## Top Posts by Community\n\n"
        
        for community in sorted_communities:
            # Sort posts by score (descending)
            sorted_posts = sorted(communities[community], key=lambda x: x.get('score', 0), reverse=True)
            
            # Take top 3 posts
            top_posts = sorted_posts[:3]
            
            # Skip if no posts
            if not top_posts:
                continue
            
            # Create table header
            all_tables += f"### r/{community}\n\n"
            all_tables += "| Title | Score | Comments | Category | Posted |\n"
            all_tables += "|-------|-------|----------|----------|--------|\n"
            
            # Add rows to table
            for post in top_posts:
                title = post.get('title', 'N/A')
                
                # Enhanced title processing to prevent Markdown formatting issues
                # 1. Replace pipe characters that would break table formatting
                title = title.replace('|', '&#124;')
                
                # 2. Escape all brackets to prevent Markdown link interpretation
                title = title.replace('[', '\\[').replace(']', '\\]')
                
                # 3. Escape quotes to prevent string interpretation issues
                title = title.replace('"', '\\"').replace("'", "\\'")
                
                # 4. Replace any newline characters or carriage returns that might be in the title
                title = title.replace('\n', ' ').replace('\r', ' ')
                
                # 5. Replace periods followed by spaces with periods and non-breaking spaces
                # to prevent line breaks after periods
                title = title.replace('. ', '.&nbsp;')
                
                # Truncate long titles
                if len(title) > 60:
                    title = title[:57] + "..."
                
                score = post.get('score', 0)
                comments = post.get('num_comments', 0)
                category = post.get('link_flair_text', 'N/A')
                if not category or category == 'None':
                    category = 'General'
                
                # Format the timestamp
                created_utc = post.get('created_utc')
                if created_utc:
                    if isinstance(created_utc, str):
                        try:
                            created_utc = datetime.fromisoformat(created_utc.replace('Z', '+00:00'))
                        except ValueError:
                            created_utc = datetime.utcnow()
                    posted_time = created_utc.strftime("%Y-%m-%d %H:%M UTC")
                else:
                    posted_time = 'N/A'
                
                # Create post URL
                post_id = post.get('post_id', '')
                post_url = f"https://www.reddit.com/comments/{post_id}"
                
                # Add row to table
                all_tables += f"| [{title}]({post_url}) | {score} | {comments} | {category} | {posted_time} |\n"
            
            # Add spacing between tables
            all_tables += "\n\n"
        
        return all_tables
    
    def generate_report(self, posts: List[Dict[str, Any]], previous_report: Optional[Dict[str, Any]] = None, 
                       weekly_posts: Optional[List[Dict[str, Any]]] = None, 
                       monthly_posts: Optional[List[Dict[str, Any]]] = None,
                       language: str = "en") -> str:
        """
        Generate a report from Reddit posts using the Groq API.
        
        Args:
            posts: List of post dictionaries (24-hour trending posts)
            previous_report: Previous report data for comparison
            weekly_posts: List of weekly popular posts
            monthly_posts: List of monthly popular posts
            language: Language for the report ('en' for English, 'zh' for Chinese)
            
        Returns:
            Generated report as a string
        """
        logger.info(f"Generating report from {len(posts)} posts using Groq API in {language} language")
        
        # Create monthly popular posts table if provided
        monthly_table = ""
        if monthly_posts:
            monthly_table = self._create_monthly_popular_table(monthly_posts, previous_report)
        
        # Create weekly popular posts table if provided
        weekly_table = ""
        if weekly_posts:
            weekly_table = self._create_weekly_popular_table(weekly_posts, previous_report)
        
        # Create trending posts table
        trending_table = self._create_trending_posts_table(posts)
        
        # Create community top posts tables
        community_tables = self._create_community_top_posts_tables(posts)
        
        # Prepare post data for analysis
        post_data = []
        for post in posts:
            post_data.append({
                "title": post.get('title', ''),
                "score": post.get('score', 0),
                "num_comments": post.get('num_comments', 0),
                "url": post.get('url', ''),
                "selftext": post.get('selftext', '')[:500] if post.get('selftext') else '',
                "created_utc": str(post.get('created_utc', '')),
                "subreddit": post.get('subreddit', ''),
                "link_flair_text": post.get('link_flair_text', '')
            })
        
        # Get current date
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        # Define language-specific content
        if language == "zh":
            # Chinese prompt and headers
            section_headers = {
                "report_title": f"# Reddit AI è¶‹åŠ¿æŠ¥å‘Š - {current_date}",
                "trending_posts": "## ä»Šæ—¥çƒ­é—¨å¸–å­",
                "weekly_posts": "## æœ¬å‘¨çƒ­é—¨å¸–å­",
                "monthly_posts": "## æœ¬æœˆçƒ­é—¨å¸–å­",
                "community_posts": "## å„ç¤¾åŒºæœ¬å‘¨çƒ­é—¨å¸–å­",
                "trend_analysis": "## è¶‹åŠ¿åˆ†æž"
            }
            
            prompt = f"""
            æ‚¨æ˜¯ä¸€ä½ä¸“é—¨åˆ†æžRedditè¶‹åŠ¿çš„AIæŠ€æœ¯åˆ†æžå¸ˆã€‚æ‚¨çš„ä»»åŠ¡æ˜¯åˆ†æžæ¥è‡ªAIç›¸å…³subredditçš„æœ€æ–°å¸–å­ï¼Œå¹¶ä¸º{current_date}ç”Ÿæˆä¸€ä»½å…¨é¢çš„æŠ¥å‘Šã€‚

            ä»¥ä¸‹æ˜¯çƒ­é—¨å’Œè¶‹åŠ¿å¸–å­çš„è¡¨æ ¼ï¼š
            
            {trending_table}
            
            {weekly_table}
            
            {monthly_table}
            
            {community_tables}
            
            æ ¹æ®è¿™äº›å¸–å­ï¼Œè¯·æä¾›ï¼š
            
            1. **ä»Šæ—¥ç„¦ç‚¹**ï¼šåˆ†æžè¿‡åŽ»24å°æ—¶å†…å‡ºçŽ°çš„æœ€æ–°è¶‹åŠ¿å’Œçªç ´æ€§å‘å±•ã€‚é‡ç‚¹å…³æ³¨ä»Šå¤©æ–°å‡ºçŽ°çš„ã€ä¸Žä¹‹å‰å‘¨è¶‹åŠ¿å’Œæœˆè¶‹åŠ¿ä¸åŒçš„æ–°å…´è¯é¢˜ã€‚å¼•ç”¨å…·ä½“çš„å¸–å­ä½œä¸ºä¾‹è¯ï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆè¿™äº›æ–°è¶‹åŠ¿å€¼å¾—å…³æ³¨ã€‚è¿™æ˜¯æŠ¥å‘Šä¸­æœ€é‡è¦çš„éƒ¨åˆ†ï¼Œåº”è¯¥å æ®æŠ¥å‘Šçš„ä¸»è¦ç¯‡å¹…ã€‚
            
            2. **å‘¨è¶‹åŠ¿å¯¹æ¯”**ï¼šå°†ä»Šæ—¥è¶‹åŠ¿ä¸Žè¿‡åŽ»ä¸€å‘¨çš„è¶‹åŠ¿è¿›è¡Œå¯¹æ¯”åˆ†æžã€‚å“ªäº›è¶‹åŠ¿æŒç»­å­˜åœ¨ï¼Ÿå“ªäº›æ˜¯æ–°å‡ºçŽ°çš„ï¼Ÿè¿™äº›å˜åŒ–åæ˜ äº†AIç¤¾åŒºå…´è¶£çš„ä»€ä¹ˆå˜åŒ–ï¼Ÿ
            
            3. **æœˆåº¦æŠ€æœ¯æ¼”è¿›**ï¼šä»Žæ›´é•¿è¿œçš„è§’åº¦ï¼Œåˆ†æžå½“å‰è¶‹åŠ¿å¦‚ä½•èžå…¥æˆ–æ”¹å˜äº†è¿‡åŽ»ä¸€ä¸ªæœˆçš„æŠ€æœ¯å‘å±•è·¯çº¿ã€‚ç‰¹åˆ«å…³æ³¨é‚£äº›å¯èƒ½ä»£è¡¨AIé¢†åŸŸé‡å¤§è½¬å˜çš„æ–°å…´æŠ€æœ¯æˆ–æ–¹æ³•ã€‚
            
            4. **æŠ€æœ¯æ·±åº¦è§£æž**ï¼šé€‰æ‹©ä¸€ä¸ªç‰¹åˆ«æœ‰è¶£æˆ–é‡è¦çš„ä»Šæ—¥è¶‹åŠ¿ï¼Œæä¾›æ›´è¯¦ç»†çš„æŠ€æœ¯è§£é‡Šï¼ŒåŒ…æ‹¬å®ƒæ˜¯ä»€ä¹ˆã€ä¸ºä»€ä¹ˆé‡è¦ã€ä»¥åŠå®ƒä¸Žæ›´å¹¿æ³›çš„AIç”Ÿæ€ç³»ç»Ÿçš„å…³ç³»ã€‚
            
            5. **ç¤¾åŒºäº®ç‚¹**ï¼šåˆ†æžè¿‡åŽ»ä¸€å‘¨å†…ä¸åŒç¤¾åŒºï¼ˆsubredditï¼‰çš„çƒ­é—¨è¯é¢˜æœ‰ä½•ä¸åŒï¼Œå„ä¸ªç¤¾åŒºå…³æ³¨çš„é‡ç‚¹æ˜¯ä»€ä¹ˆï¼Œä»¥åŠè¿™äº›ç¤¾åŒºä¹‹é—´çš„äº¤å‰è¯é¢˜ã€‚ç‰¹åˆ«å…³æ³¨é‚£äº›åœ¨å¤§åž‹ç¤¾åŒºä¹‹å¤–çš„å°åž‹ç¤¾åŒºä¸­å‡ºçŽ°çš„ç‹¬ç‰¹è®¨è®ºå’Œè§è§£ã€‚
            
            æ‚¨çš„åˆ†æžåº”è¯¥ç®€æ´ã€æœ‰è§åœ°ï¼Œå¹¶ä¸“æ³¨äºŽå¯¹AIä¸“ä¸šäººå£«æœ‰ç”¨çš„å¯æ“ä½œä¿¡æ¯ã€‚é¿å…ä¸€èˆ¬æ€§é™ˆè¿°ï¼Œè€Œæ˜¯æä¾›åŸºäºŽå¸–å­æ•°æ®çš„å…·ä½“è§è§£ã€‚ç‰¹åˆ«å¼ºè°ƒä»Šæ—¥çš„æ–°å‘çŽ°å’Œçªç ´ï¼ŒåŒæ—¶å°†å…¶æ”¾åœ¨å‘¨è¶‹åŠ¿å’Œæœˆè¶‹åŠ¿çš„èƒŒæ™¯ä¸‹è¿›è¡Œåˆ†æžã€‚
            
            è¯·ç”¨ä¸­æ–‡å›žç­”ã€‚
            """
        else:
            # English (default) prompt and headers
            section_headers = {
                "report_title": f"# Reddit AI Trend Report - {current_date}",
                "trending_posts": "## Today's Trending Posts",
                "weekly_posts": "## Weekly Popular Posts",
                "monthly_posts": "## Monthly Popular Posts",
                "community_posts": "## Top Posts by Community (Past Week)",
                "trend_analysis": "## Trend Analysis"
            }
            
            prompt = f"""
            You are an AI technology analyst specializing in Reddit trend analysis. Your task is to analyze recent posts from AI-related subreddits and generate a comprehensive report for {current_date}.

            Below are the tables of popular and trending posts:
            
            {trending_table}
            
            {weekly_table}
            
            {monthly_table}
            
            {community_tables}
            
            Based on these posts, please provide:
            
            1. **Today's Highlights**: Analyze the latest trends and breakthrough developments that have emerged in the past 24 hours. Focus on new emerging topics that differ from the previous weekly and monthly trends. Reference specific posts as examples and explain why these new trends are worth attention. This is the most important part of the report and should occupy the main portion.
            
            2. **Weekly Trend Comparison**: Compare today's trends with those from the past week. Which trends persist? Which ones are newly emerging? What do these changes reflect about shifting interests in the AI community?
            
            3. **Monthly Technology Evolution**: From a longer-term perspective, analyze how current trends fit into or change the technological development path of the past month. Pay special attention to emerging technologies or methods that may represent significant shifts in the AI field.
            
            4. **Technical Deep Dive**: Select a particularly interesting or important trend from today and provide a more detailed technical explanation, including what it is, why it's important, and its relationship to the broader AI ecosystem.
            
            5. **Community Highlights**: Analyze how the hot topics from the past week differ across communities (subreddits), what each community is focusing on, and what cross-cutting topics appear across communities. Pay special attention to unique discussions and insights emerging from smaller communities beyond the major ones.
            
            Your analysis should be concise, insightful, and focused on actionable information useful to AI professionals. Avoid general statements and instead provide specific insights based on the post data. Particularly emphasize today's new discoveries and breakthroughs, while analyzing them in the context of weekly and monthly trends.
            
            Please respond in English.
            """
        
        try:
            # Generate the report
            report = self.generate_text(prompt)
            
            # Add header and tables
            full_report = f"{section_headers['report_title']}\n\n"
            full_report += f"{section_headers['trending_posts']}\n\n"
            full_report += trending_table.replace("## Trending Posts - Last 24 Hours\n\n", "") + "\n\n"
            full_report += f"{section_headers['weekly_posts']}\n\n"
            full_report += weekly_table.replace("## Weekly Popular Posts\n\n", "") + "\n\n"
            full_report += f"{section_headers['monthly_posts']}\n\n"
            full_report += monthly_table.replace("## Monthly Popular Posts\n\n", "") + "\n\n"
            full_report += f"{section_headers['community_posts']}\n\n"
            full_report += community_tables.replace("## Top Posts by Community\n\n", "") + "\n\n"
            full_report += f"{section_headers['trend_analysis']}\n\n"
            full_report += report
            
            return full_report
        except Exception as e:
            logger.error(f"Error generating report: {e}")
            return f"Error generating report: {e}"
            
    def generate_multilingual_reports(self, posts: List[Dict[str, Any]], previous_data: Optional[Dict[str, Any]] = None, 
                                     weekly_posts: Optional[List[Dict[str, Any]]] = None, 
                                     monthly_posts: Optional[List[Dict[str, Any]]] = None,
                                     languages: List[str] = ["en", "zh"]) -> Dict[str, str]:
        """
        Generate reports in multiple languages.
        
        Args:
            posts: List of post dictionaries (24-hour trending posts)
            previous_data: Optional dictionary containing previous report data for comparison
            weekly_posts: List of weekly popular posts
            monthly_posts: List of monthly popular posts
            languages: List of language codes to generate reports for
            
        Returns:
            Dictionary mapping language codes to generated reports
        """
        reports = {}
        
        for lang in languages:
            logger.info(f"Generating report in {lang} language")
            report = self.generate_report(posts, previous_data, weekly_posts, monthly_posts, language=lang)
            reports[lang] = report
            
        return reports 